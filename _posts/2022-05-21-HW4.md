---
layout: post
title:  "Classifying Fake News with Tensorflow"
author: Kelly Song
---

Hello everyone! Today we will be learning how to classify fake news with Tensorflow! Let's begin by importing the necessary packages and data.

# Importing necessary packages and data


```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

from tensorflow.keras import layers
from tensorflow.keras import losses

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

train_data = pd.read_csv(train_url)
train_data
```


The data we are importing contains article titles, text, and a column telling us if they are fake news or not.


  <div id="df-bac15cee-d607-4ffc-96af-2f68404375a4">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
      </div>
  <button class="colab-df-convert" onclick="convertToInteractive('df-bac15cee-d607-4ffc-96af-2f68404375a4')"
          title="Convert this dataframe to an interactive table."
          style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
   width="24px">
<path d="M0 0h24v24H0V0z" fill="none"/>
<path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
</svg>
  </button>

</div>
</div>

Next, we will be making a function to turn our data into a tensor object! This will allow us to create our models.

# Make function to make data tensor object

Our function will first remove stopwords from our titles and text, and then we will turn it into a tensor object. Title and text will be our input, and fake will be our output.

```python
def make_dataset(df):
  # Remove stopwords from the article text and title. 
  # A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” 
  # Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), 
  # and the output should consist only of the fake column. You may find it helpful to consult lecture notes or this tutorial for 
  # reference on how to construct and use Datasets with multiple inputs

  #stop words
  stop = stopwords.words('english')

  #remove stop words from article title
  df["title"] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  #remove stop words from article text
  df["text"] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  data = tf.data.Dataset.from_tensor_slices(
   ( # dictionary for input data/features
    #our two inputs: title and text
    { "title": df[["title"]],
     "text": df["text"]
    },
    # dictionary for output data/labels
    # one output: fake
    { "fake": df[["fake"]]
        
    }   
   ) 
  ) 

  return data.batch(100)

```


```python
#batch our data to make training faster; train on chunks of data rather than individual rows
data = make_dataset(train_data)
```


```python
#check size of dataset
len(data)
```




    225


Now, we will be splitting our data into a training and validation set. Our validation set will be 20% of the data.

# Split data into training and validation sets


```python
#shuffle dataset
import random
random.seed(10)
data = data.shuffle(buffer_size = len(data))

#20% validation
val_size  = int(0.2*len(data))

val = data.take(val_size)
train = data.skip(val_size).take(len(data) - val_size)

#check size of training and validation sets
print(len(train), len(val))
```

    180 45


# Base rate - Labels Iterator, fake text, count of labels, on training data

Now, let's look at our base rate by looking at the "fake" labels in our training data.


```python
# Base rate 
## similar to previous hw; can get true and fake from training data, labels iterator on fake column

labels_iterator= train.unbatch().map(lambda input, output: output).as_numpy_iterator()

train_data2 = train_data.sample(n=1800)

len(train_data2[train_data2["fake"] == 1]) / 1800
```




    0.5333333333333333



The base rate appears to be about 53%, which indicates that a little more than half of the artiles in the training data is fake while the other half is true.

# Model Creation

Next we will create our models, but before we do so, we must perform standardization and text vectorization.

```python
# Text vectorization

#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

## Title Vectorization
title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))

## Text Vectorization

text_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, 
    output_mode='int',
    output_sequence_length=500) 

text_vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```

We will be creating 3 models: one with the title only, one with the text only, and one with both the text and title. The purpose of this is to see which method will classify fake news the best.

### Model 1: Title only

For our first model, we will focus on just using the title. We must first create an input.


```python
title_input = tf.keras.Input(
    shape=(1,),
    name = "title", # same name as the dictionary key in the dataset
    dtype = "string"
)
```
Next, we will add our layers. Since our titles are categorical and not numerical, we will add GlobalAveragePooling. For this, we will include an embedding and begin with 0.2 for our dropout. I decided to begin with these parameters based on the lecture notes.

```python
# layers for processing the title
title_features = title_vectorize_layer(title_input)

# Add embedding layer, dropout
title_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(2, activation='relu')(title_features)

output = layers.Dense(2, name="fake")(title_features) 
```

Now, we create our model and check its summary.


```python
model1 = tf.keras.Model(
    inputs = title_input,
    outputs = output
)
```


```python
from tensorflow.keras import utils

model1.summary()
utils.plot_model(model1)
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding1 (Embedding)      (None, 500, 3)            6000      
                                                                     
     dropout (Dropout)           (None, 500, 3)            0         
                                                                     
     global_average_pooling1d (G  (None, 3)                0         
     lobalAveragePooling1D)                                          
                                                                     
     dropout_1 (Dropout)         (None, 3)                 0         
                                                                     
     dense (Dense)               (None, 2)                 8         
                                                                     
     fake (Dense)                (None, 2)                 6         
                                                                     
    =================================================================
    Total params: 6,014
    Trainable params: 6,014
    Non-trainable params: 0
    _________________________________________________________________





    
![output_18_1.png](/images/output_18_1.png)


```python
model1.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```

Now, let's fit our data.

```python
history = model1.fit(train, validation_data=val, epochs=20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 2s 9ms/step - loss: 0.6919 - accuracy: 0.5194 - val_loss: 0.6910 - val_accuracy: 0.5187
    Epoch 2/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.6889 - accuracy: 0.5248 - val_loss: 0.6873 - val_accuracy: 0.5140
    Epoch 3/20
    180/180 [==============================] - 1s 8ms/step - loss: 0.6828 - accuracy: 0.5826 - val_loss: 0.6756 - val_accuracy: 0.5307
    Epoch 4/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.6671 - accuracy: 0.7153 - val_loss: 0.6536 - val_accuracy: 0.9489
    Epoch 5/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.6372 - accuracy: 0.8050 - val_loss: 0.6132 - val_accuracy: 0.8904
    Epoch 6/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.5865 - accuracy: 0.8598 - val_loss: 0.5551 - val_accuracy: 0.8987
    Epoch 7/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.5241 - accuracy: 0.8771 - val_loss: 0.4806 - val_accuracy: 0.9418
    Epoch 8/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.4522 - accuracy: 0.9106 - val_loss: 0.4145 - val_accuracy: 0.9453
    Epoch 9/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.3883 - accuracy: 0.9278 - val_loss: 0.3423 - val_accuracy: 0.9508
    Epoch 10/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.3353 - accuracy: 0.9366 - val_loss: 0.2915 - val_accuracy: 0.9549
    Epoch 11/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.2937 - accuracy: 0.9402 - val_loss: 0.2577 - val_accuracy: 0.9540
    Epoch 12/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.2595 - accuracy: 0.9445 - val_loss: 0.2223 - val_accuracy: 0.9562
    Epoch 13/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.2318 - accuracy: 0.9496 - val_loss: 0.1933 - val_accuracy: 0.9669
    Epoch 14/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.2084 - accuracy: 0.9526 - val_loss: 0.1802 - val_accuracy: 0.9602
    Epoch 15/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.1905 - accuracy: 0.9540 - val_loss: 0.1583 - val_accuracy: 0.9622
    Epoch 16/20
    180/180 [==============================] - 1s 6ms/step - loss: 0.1741 - accuracy: 0.9569 - val_loss: 0.1443 - val_accuracy: 0.9702
    Epoch 17/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.1637 - accuracy: 0.9571 - val_loss: 0.1306 - val_accuracy: 0.9698
    Epoch 18/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.1522 - accuracy: 0.9603 - val_loss: 0.1208 - val_accuracy: 0.9698
    Epoch 19/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.1403 - accuracy: 0.9614 - val_loss: 0.1106 - val_accuracy: 0.9748
    Epoch 20/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.1330 - accuracy: 0.9624 - val_loss: 0.1017 - val_accuracy: 0.9731


As we can see, we have a pretty good validation accuracy (around 97%) and the models don't appear to be overfitted! Let's visualize this process then move onto our next model.

```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
    
![output_21_2.png](/images/output_21_2.png)
    


### Model 2: Text Only

Our next model is using just text. We repeat the same process as above, but with the text instead of titles.

```python
text_input = tf.keras.Input(
    shape=(1,),
    name = "text",
    dtype = "string"
)
```

For our layers, I initially tried the same parameters as the title model, but decided to try 0.3 and 0.4 for the dropout because the model seemed to be a bit overfitted. 0.4 had the best accuracy rates, so I went with that one! After this, let's fit our model and visualize it.

```python
# layers for processing the title
text_features = text_vectorize_layer(text_input)

# Add embedding layer, dropout
text_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding2")(text_features)
text_features = layers.Dropout(0.4)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.4)(text_features)
text_features = layers.Dense(2, activation='relu')(text_features)

output = layers.Dense(2, name="fake")(text_features) 
```


```python
model2 = tf.keras.Model(
    inputs = text_input,
    outputs = output
)
```


```python
from tensorflow.keras import utils

model2.summary()
utils.plot_model(model2)
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization_1 (TextV  (None, 500)              0         
     ectorization)                                                   
                                                                     
     embedding2 (Embedding)      (None, 500, 3)            6000      
                                                                     
     dropout_2 (Dropout)         (None, 500, 3)            0         
                                                                     
     global_average_pooling1d_1   (None, 3)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_3 (Dropout)         (None, 3)                 0         
                                                                     
     dense_1 (Dense)             (None, 2)                 8         
                                                                     
     fake (Dense)                (None, 2)                 6         
                                                                     
    =================================================================
    Total params: 6,014
    Trainable params: 6,014
    Non-trainable params: 0
    _________________________________________________________________





    
![output_26_1.png](/images/output_26_1.png)
    


```python
model2.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```


```python
history = model2.fit(train, validation_data=val, epochs=20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 3s 14ms/step - loss: 0.6848 - accuracy: 0.5263 - val_loss: 0.6711 - val_accuracy: 0.5304
    Epoch 2/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.6433 - accuracy: 0.6149 - val_loss: 0.6008 - val_accuracy: 0.7107
    Epoch 3/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.5630 - accuracy: 0.8170 - val_loss: 0.5032 - val_accuracy: 0.9104
    Epoch 4/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.4802 - accuracy: 0.8793 - val_loss: 0.4182 - val_accuracy: 0.9347
    Epoch 5/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.4107 - accuracy: 0.8972 - val_loss: 0.3536 - val_accuracy: 0.9425
    Epoch 6/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.3582 - accuracy: 0.9138 - val_loss: 0.2917 - val_accuracy: 0.9591
    Epoch 7/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.3135 - accuracy: 0.9213 - val_loss: 0.2511 - val_accuracy: 0.9600
    Epoch 8/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.2852 - accuracy: 0.9229 - val_loss: 0.2223 - val_accuracy: 0.9652
    Epoch 9/20
    180/180 [==============================] - 2s 12ms/step - loss: 0.2597 - accuracy: 0.9270 - val_loss: 0.1985 - val_accuracy: 0.9670
    Epoch 10/20
    180/180 [==============================] - 2s 12ms/step - loss: 0.2383 - accuracy: 0.9325 - val_loss: 0.1800 - val_accuracy: 0.9682
    Epoch 11/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.2216 - accuracy: 0.9342 - val_loss: 0.1620 - val_accuracy: 0.9690
    Epoch 12/20
    180/180 [==============================] - 3s 19ms/step - loss: 0.2122 - accuracy: 0.9361 - val_loss: 0.1585 - val_accuracy: 0.9656
    Epoch 13/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.1983 - accuracy: 0.9355 - val_loss: 0.1493 - val_accuracy: 0.9684
    Epoch 14/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.1889 - accuracy: 0.9361 - val_loss: 0.1349 - val_accuracy: 0.9736
    Epoch 15/20
    180/180 [==============================] - 2s 12ms/step - loss: 0.1787 - accuracy: 0.9401 - val_loss: 0.1198 - val_accuracy: 0.9742
    Epoch 16/20
    180/180 [==============================] - 2s 12ms/step - loss: 0.1729 - accuracy: 0.9381 - val_loss: 0.1146 - val_accuracy: 0.9758
    Epoch 17/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.1671 - accuracy: 0.9375 - val_loss: 0.1161 - val_accuracy: 0.9767
    Epoch 18/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.1623 - accuracy: 0.9367 - val_loss: 0.1058 - val_accuracy: 0.9760
    Epoch 19/20
    180/180 [==============================] - 2s 12ms/step - loss: 0.1578 - accuracy: 0.9418 - val_loss: 0.1106 - val_accuracy: 0.9713
    Epoch 20/20
    180/180 [==============================] - 2s 12ms/step - loss: 0.1574 - accuracy: 0.9408 - val_loss: 0.0985 - val_accuracy: 0.9796


As we can see, the validation accuracy is pretty high, sitting at around 98%! This looks promising, but let's try our final model.

```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

    No handles with labels found to put in legend.





    <matplotlib.legend.Legend at 0x7fca5eed35d0>




    
![output_29_2.png](/images/output_29_2.png)
    


### Model 3: Title and Text

Now, we create a model that uses both the title and text. We begin by concatenating our title and text feattures.


```python
main = layers.concatenate([title_features, text_features], axis = 1)
```

Next, we will create layers for this concatenation and use it for our output. 

```python
main = layers.Dense(4, activation='relu')(main)
output = layers.Dense(4, name="fake")(main) 
```


```python
model3 = tf.keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```


```python
model3.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```


```python
history = model3.fit(train, 
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20
    180/180 [==============================] - 4s 17ms/step - loss: 1.0917 - accuracy: 0.5142 - val_loss: 0.8879 - val_accuracy: 0.5180
    Epoch 2/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.7575 - accuracy: 0.5338 - val_loss: 0.6110 - val_accuracy: 0.6187
    Epoch 3/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.5299 - accuracy: 0.7800 - val_loss: 0.3960 - val_accuracy: 0.9418
    Epoch 4/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.3369 - accuracy: 0.9180 - val_loss: 0.2088 - val_accuracy: 0.9716
    Epoch 5/20
    180/180 [==============================] - 3s 15ms/step - loss: 0.2234 - accuracy: 0.9288 - val_loss: 0.1369 - val_accuracy: 0.9762
    Epoch 6/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1859 - accuracy: 0.9348 - val_loss: 0.1102 - val_accuracy: 0.9778
    Epoch 7/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1673 - accuracy: 0.9360 - val_loss: 0.0939 - val_accuracy: 0.9816
    Epoch 8/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1609 - accuracy: 0.9354 - val_loss: 0.0905 - val_accuracy: 0.9796
    Epoch 9/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1524 - accuracy: 0.9366 - val_loss: 0.0907 - val_accuracy: 0.9767
    Epoch 10/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1500 - accuracy: 0.9372 - val_loss: 0.0912 - val_accuracy: 0.9760
    Epoch 11/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1465 - accuracy: 0.9388 - val_loss: 0.0862 - val_accuracy: 0.9787
    Epoch 12/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1404 - accuracy: 0.9403 - val_loss: 0.0772 - val_accuracy: 0.9796
    Epoch 13/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1377 - accuracy: 0.9412 - val_loss: 0.0760 - val_accuracy: 0.9800
    Epoch 14/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1339 - accuracy: 0.9444 - val_loss: 0.0783 - val_accuracy: 0.9789
    Epoch 15/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1337 - accuracy: 0.9413 - val_loss: 0.0695 - val_accuracy: 0.9820
    Epoch 16/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1290 - accuracy: 0.9441 - val_loss: 0.0664 - val_accuracy: 0.9858
    Epoch 17/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1283 - accuracy: 0.9460 - val_loss: 0.0639 - val_accuracy: 0.9824
    Epoch 18/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1261 - accuracy: 0.9458 - val_loss: 0.0579 - val_accuracy: 0.9851
    Epoch 19/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1285 - accuracy: 0.9442 - val_loss: 0.0536 - val_accuracy: 0.9882
    Epoch 20/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1243 - accuracy: 0.9458 - val_loss: 0.0649 - val_accuracy: 0.9858


As we can see, this model performed the best, sitting at 98.6%!

```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])

#blue is training
#orange is validation
```




    [<matplotlib.lines.Line2D at 0x7fca601041d0>]




    
![output_36_1.png](/images/output_36_1.png)
    


Looking at all of our models, it appears that the model using both text and title scored the highest and is our best model! This makes sense, as using both the text and title gives our model more information to learn from and thus may be more helpful. Let's evaluate our model using test data and see how it performs!

### Model Evaluation on Test Data

Let's read in our test data.

```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"

test_data = pd.read_csv(test_url)
test_data

test = make_dataset(test_data)
```

Now, we will train our best model (model 3) on our test data and see how it performs.

```python
history = model2.fit(train, validation_data=test, epochs=20)
```

```python
history = model3.fit(train, validation_data=test, epochs=20)
```

    Epoch 1/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1273 - accuracy: 0.9433 - val_loss: 0.0861 - val_accuracy: 0.9771
    Epoch 2/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1193 - accuracy: 0.9450 - val_loss: 0.0854 - val_accuracy: 0.9772
    Epoch 3/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1205 - accuracy: 0.9469 - val_loss: 0.0866 - val_accuracy: 0.9779
    Epoch 4/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1182 - accuracy: 0.9474 - val_loss: 0.0830 - val_accuracy: 0.9775
    Epoch 5/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1175 - accuracy: 0.9487 - val_loss: 0.0832 - val_accuracy: 0.9779
    Epoch 6/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1174 - accuracy: 0.9496 - val_loss: 0.0822 - val_accuracy: 0.9772
    Epoch 7/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1147 - accuracy: 0.9486 - val_loss: 0.0813 - val_accuracy: 0.9768
    Epoch 8/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1161 - accuracy: 0.9464 - val_loss: 0.0823 - val_accuracy: 0.9764
    Epoch 9/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1156 - accuracy: 0.9463 - val_loss: 0.0798 - val_accuracy: 0.9783
    Epoch 10/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1110 - accuracy: 0.9477 - val_loss: 0.0795 - val_accuracy: 0.9784
    Epoch 11/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1129 - accuracy: 0.9486 - val_loss: 0.0789 - val_accuracy: 0.9788
    Epoch 12/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1129 - accuracy: 0.9494 - val_loss: 0.0795 - val_accuracy: 0.9787
    Epoch 13/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1136 - accuracy: 0.9472 - val_loss: 0.0787 - val_accuracy: 0.9793
    Epoch 14/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1074 - accuracy: 0.9527 - val_loss: 0.0778 - val_accuracy: 0.9788
    Epoch 15/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1084 - accuracy: 0.9513 - val_loss: 0.0781 - val_accuracy: 0.9787
    Epoch 16/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1105 - accuracy: 0.9491 - val_loss: 0.0778 - val_accuracy: 0.9794
    Epoch 17/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1104 - accuracy: 0.9514 - val_loss: 0.0772 - val_accuracy: 0.9797
    Epoch 18/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1099 - accuracy: 0.9478 - val_loss: 0.0769 - val_accuracy: 0.9800
    Epoch 19/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1125 - accuracy: 0.9500 - val_loss: 0.0785 - val_accuracy: 0.9793
    Epoch 20/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.1057 - accuracy: 0.9526 - val_loss: 0.0785 - val_accuracy: 0.9788


Our model did pretty well, scoring around a 98% for our testing accuracy rate!

```python
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()

#blue is training
#orange is validation
```
    
![output_40_2.png](/images/output_40_2.png)
    
Now, let's create an embedding visualization.

### Embedding Visualization


```python
weights = model2.get_layer('embedding2').get_weights()[0] # get the weights from the embedding layer
vocab = text_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = [2]*len(embedding_df),
                # size_max = 2,
                 hover_name = "word")

fig.show()
```

![word_plot.png](/images/word_plot.png)

Five words that I found interpretable and interesting in this visualization are "apparently," "fox," "reportedly," "radical," and "21wire," all of which reside towards the left side of the visualization. "fox" and "21wire" most likely refers to Fox News and 21st Century Wire, which are both news sources that have been criticized for spreading propoganda and false or exhaggerated information. "apparently" and "reportedly" also make sense to me because these are words that are often used by writers when they can't be sure about the information; these words allow them to detach themselves from involvement. "radical" also makes sense as a fake news word because a lot of fake news articles tend to attack "radical leftists."



That's it for today! Thank you so much for reading!

