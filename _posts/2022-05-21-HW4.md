---
layout: post
title:  "Classifying Fake News with Tensorflow"
author: Kelly Song
---

# Importing necessary packages and data



```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

from tensorflow.keras import layers
from tensorflow.keras import losses

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

train_data = pd.read_csv(train_url)
train_data
```





  <div id="df-bac15cee-d607-4ffc-96af-2f68404375a4">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>

# Make function to clean data


```python
def make_dataset(df):
  # Remove stopwords from the article text and title. 
  # A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” 
  # Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), 
  # and the output should consist only of the fake column. You may find it helpful to consult lecture notes or this tutorial for 
  # reference on how to construct and use Datasets with multiple inputs

  #stop words
  stop = stopwords.words('english')

  #remove stop words from article title
  df["title"] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  #remove stop words from article text
  df["text"] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  data = tf.data.Dataset.from_tensor_slices(
   ( # dictionary for input data/features
    #our two inputs: title and text
    { "title": df[["title"]],
     "text": df["text"]
    },
    # dictionary for output data/labels
    # one output: fake
    { "fake": df[["fake"]]
        
    }   
   ) 
  ) 

  return data.batch(100)

```


```python
#batch our data to make training faster; train on chunks of data rather than individual rows
data = make_dataset(train_data)
```


```python
#check size of dataset
len(data)
```




    225



# Split data into training and validation sets


```python
#shuffle dataset
data = data.shuffle(buffer_size = len(data))

#20% validation
val_size  = int(0.2*len(data))

val = data.take(val_size)
train = data.skip(val_size).take(len(data) - val_size)

#check size of training and validation sets
print(len(train), len(val))
```

    180 45


# Base rate - Labels Iterator, fake text, count of labels, on training data


```python
# Base rate 
## similar to previous hw; can get true and fake from training data, labels iterator on fake column

labels_iterator= train.unbatch().map(lambda input, output: output).as_numpy_iterator()

train_data2 = train_data.sample(n=1800)

len(train_data2[train_data2["fake"] == 1]) / 1800
```




    0.5211111111111111



The base rate appears to be about 52%, which indicates that a little more than half of the artiles in the training data is fake while the other half is true.

# Model Creation


```python
# Text vectorization

#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

## Title Vectorization
title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))

## Text Vectorization

text_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, 
    output_mode='int',
    output_sequence_length=500) 

text_vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```

### Model 1: Title only


```python
title_input = tf.keras.Input(
    shape=(1,),
    name = "title", # same name as the dictionary key in the dataset
    dtype = "string"
)
```


```python
# layers for processing the title
title_features = title_vectorize_layer(title_input)

# Add embedding layer, dropout
title_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(2, activation='relu')(title_features)

output = layers.Dense(2, name="fake")(title_features) 
```


```python
model1 = tf.keras.Model(
    inputs = title_input,
    outputs = output
)
```


```python
from tensorflow.keras import utils

model1.summary()
utils.plot_model(model1)
```

    Model: "model_14"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization_11 (Text  (None, 500)              0         
     Vectorization)                                                  
                                                                     
     embedding1 (Embedding)      (None, 500, 3)            6000      
                                                                     
     dropout_21 (Dropout)        (None, 500, 3)            0         
                                                                     
     global_average_pooling1d_9   (None, 3)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_22 (Dropout)        (None, 3)                 0         
                                                                     
     dense_16 (Dense)            (None, 2)                 8         
                                                                     
     fake (Dense)                (None, 2)                 6         
                                                                     
    =================================================================
    Total params: 6,014
    Trainable params: 6,014
    Non-trainable params: 0
    _________________________________________________________________





    
![output_18_1.png](/images/output_18_1.png)


```python
model1.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```


```python
history = model1.fit(train, validation_data=val, epochs=20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 3s 12ms/step - loss: 0.6923 - accuracy: 0.5243 - val_loss: 0.6922 - val_accuracy: 0.5215
    Epoch 2/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.6918 - accuracy: 0.5250 - val_loss: 0.6920 - val_accuracy: 0.5238
    Epoch 3/20
    180/180 [==============================] - 2s 11ms/step - loss: 0.6899 - accuracy: 0.5240 - val_loss: 0.6852 - val_accuracy: 0.5322
    Epoch 4/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.6802 - accuracy: 0.6257 - val_loss: 0.6699 - val_accuracy: 0.5476
    Epoch 5/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.6578 - accuracy: 0.7764 - val_loss: 0.6413 - val_accuracy: 0.8995
    Epoch 6/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.6224 - accuracy: 0.8418 - val_loss: 0.5980 - val_accuracy: 0.9313
    Epoch 7/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.5781 - accuracy: 0.8676 - val_loss: 0.5499 - val_accuracy: 0.9416
    Epoch 8/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.5285 - accuracy: 0.8818 - val_loss: 0.4960 - val_accuracy: 0.9387
    Epoch 9/20
    180/180 [==============================] - 2s 11ms/step - loss: 0.4783 - accuracy: 0.9055 - val_loss: 0.4402 - val_accuracy: 0.9470
    Epoch 10/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.4315 - accuracy: 0.9146 - val_loss: 0.3963 - val_accuracy: 0.9422
    Epoch 11/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.3909 - accuracy: 0.9266 - val_loss: 0.3490 - val_accuracy: 0.9520
    Epoch 12/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.3517 - accuracy: 0.9338 - val_loss: 0.3149 - val_accuracy: 0.9559
    Epoch 13/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.3183 - accuracy: 0.9396 - val_loss: 0.2812 - val_accuracy: 0.9573
    Epoch 14/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.2909 - accuracy: 0.9437 - val_loss: 0.2522 - val_accuracy: 0.9580
    Epoch 15/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.2654 - accuracy: 0.9472 - val_loss: 0.2289 - val_accuracy: 0.9602
    Epoch 16/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.2418 - accuracy: 0.9540 - val_loss: 0.2095 - val_accuracy: 0.9642
    Epoch 17/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.2243 - accuracy: 0.9528 - val_loss: 0.1882 - val_accuracy: 0.9644
    Epoch 18/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.2084 - accuracy: 0.9549 - val_loss: 0.1772 - val_accuracy: 0.9609
    Epoch 19/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.1949 - accuracy: 0.9564 - val_loss: 0.1663 - val_accuracy: 0.9707
    Epoch 20/20
    180/180 [==============================] - 2s 10ms/step - loss: 0.1806 - accuracy: 0.9582 - val_loss: 0.1520 - val_accuracy: 0.9644



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

    No handles with labels found to put in legend.





    <matplotlib.legend.Legend at 0x7fca5e6ad650>




    
![output_21_2.png](/images/output_21_2.png)
    


### Model 2: Text Only


```python
text_input = tf.keras.Input(
    shape=(1,),
    name = "text",
    dtype = "string"
)
```


```python
# layers for processing the title
text_features = text_vectorize_layer(text_input)

# Add embedding layer, dropout
text_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding2")(text_features)
text_features = layers.Dropout(0.4)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.4)(text_features)
text_features = layers.Dense(2, activation='relu')(text_features)

output = layers.Dense(2, name="fake")(text_features) 
```


```python
model2 = tf.keras.Model(
    inputs = text_input,
    outputs = output
)
```


```python
from tensorflow.keras import utils

model2.summary()
utils.plot_model(model2)
```

    Model: "model_18"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization_12 (Text  (None, 500)              0         
     Vectorization)                                                  
                                                                     
     embedding2 (Embedding)      (None, 500, 3)            6000      
                                                                     
     dropout_27 (Dropout)        (None, 500, 3)            0         
                                                                     
     global_average_pooling1d_12  (None, 3)                0         
      (GlobalAveragePooling1D)                                       
                                                                     
     dropout_28 (Dropout)        (None, 3)                 0         
                                                                     
     dense_20 (Dense)            (None, 2)                 8         
                                                                     
     fake (Dense)                (None, 2)                 6         
                                                                     
    =================================================================
    Total params: 6,014
    Trainable params: 6,014
    Non-trainable params: 0
    _________________________________________________________________





    
![output_21_1.png](/images/output_26_1.png)
    




```python
model2.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```


```python
history = model2.fit(train, validation_data=val, epochs=20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning:
    
    Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
    


    180/180 [==============================] - 4s 19ms/step - loss: 0.6851 - accuracy: 0.6272 - val_loss: 0.6678 - val_accuracy: 0.8378
    Epoch 2/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.6340 - accuracy: 0.8229 - val_loss: 0.5844 - val_accuracy: 0.9224
    Epoch 3/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.5501 - accuracy: 0.8736 - val_loss: 0.4925 - val_accuracy: 0.9133
    Epoch 4/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.4661 - accuracy: 0.8911 - val_loss: 0.4028 - val_accuracy: 0.9470
    Epoch 5/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.3975 - accuracy: 0.9092 - val_loss: 0.3386 - val_accuracy: 0.9496
    Epoch 6/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.3480 - accuracy: 0.9148 - val_loss: 0.2847 - val_accuracy: 0.9564
    Epoch 7/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.3067 - accuracy: 0.9218 - val_loss: 0.2510 - val_accuracy: 0.9607
    Epoch 8/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.2766 - accuracy: 0.9270 - val_loss: 0.2159 - val_accuracy: 0.9622
    Epoch 9/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.2526 - accuracy: 0.9307 - val_loss: 0.1993 - val_accuracy: 0.9622
    Epoch 10/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.2331 - accuracy: 0.9351 - val_loss: 0.1726 - val_accuracy: 0.9713
    Epoch 11/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.2193 - accuracy: 0.9357 - val_loss: 0.1606 - val_accuracy: 0.9683
    Epoch 12/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.2051 - accuracy: 0.9348 - val_loss: 0.1463 - val_accuracy: 0.9744
    Epoch 13/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.1936 - accuracy: 0.9390 - val_loss: 0.1349 - val_accuracy: 0.9749
    Epoch 14/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1841 - accuracy: 0.9404 - val_loss: 0.1273 - val_accuracy: 0.9756
    Epoch 15/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.1801 - accuracy: 0.9397 - val_loss: 0.1233 - val_accuracy: 0.9762
    Epoch 16/20
    180/180 [==============================] - 3s 18ms/step - loss: 0.1763 - accuracy: 0.9395 - val_loss: 0.1154 - val_accuracy: 0.9751
    Epoch 17/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1696 - accuracy: 0.9411 - val_loss: 0.1137 - val_accuracy: 0.9758
    Epoch 18/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.1611 - accuracy: 0.9431 - val_loss: 0.0976 - val_accuracy: 0.9791
    Epoch 19/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.1595 - accuracy: 0.9404 - val_loss: 0.1005 - val_accuracy: 0.9767
    Epoch 20/20
    180/180 [==============================] - 3s 16ms/step - loss: 0.1516 - accuracy: 0.9413 - val_loss: 0.0921 - val_accuracy: 0.9813



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

    No handles with labels found to put in legend.





    <matplotlib.legend.Legend at 0x7fca5eed35d0>




    
![output_29_2.png](/images/output_29_2.png)
    


### Model 3: Title and Text


```python
main = layers.concatenate([title_features, text_features], axis = 1)
```


```python
main = layers.Dense(2, activation='relu')(main)
output = layers.Dense(2, name="fake")(main) 
```


```python
model3 = tf.keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```


```python
model3.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```


```python
history = model3.fit(train, 
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20
    180/180 [==============================] - 60s 22ms/step - loss: 0.7790 - accuracy: 0.3379 - val_loss: 0.7082 - val_accuracy: 0.4678
    Epoch 2/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.7002 - accuracy: 0.4953 - val_loss: 0.6964 - val_accuracy: 0.5044
    Epoch 3/20
    180/180 [==============================] - 4s 20ms/step - loss: 0.6941 - accuracy: 0.5139 - val_loss: 0.6924 - val_accuracy: 0.5213
    Epoch 4/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6925 - accuracy: 0.5208 - val_loss: 0.6916 - val_accuracy: 0.5287
    Epoch 5/20
    180/180 [==============================] - 4s 20ms/step - loss: 0.6922 - accuracy: 0.5225 - val_loss: 0.6916 - val_accuracy: 0.5287
    Epoch 6/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6920 - accuracy: 0.5243 - val_loss: 0.6907 - val_accuracy: 0.5389
    Epoch 7/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6923 - accuracy: 0.5215 - val_loss: 0.6915 - val_accuracy: 0.5289
    Epoch 8/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6921 - accuracy: 0.5237 - val_loss: 0.6921 - val_accuracy: 0.5226
    Epoch 9/20
    180/180 [==============================] - 4s 20ms/step - loss: 0.6922 - accuracy: 0.5226 - val_loss: 0.6921 - val_accuracy: 0.5224
    Epoch 10/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6923 - accuracy: 0.5208 - val_loss: 0.6929 - val_accuracy: 0.5127
    Epoch 11/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6923 - accuracy: 0.5209 - val_loss: 0.6930 - val_accuracy: 0.5107
    Epoch 12/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.6921 - accuracy: 0.5233 - val_loss: 0.6933 - val_accuracy: 0.5098
    Epoch 13/20
    180/180 [==============================] - 4s 20ms/step - loss: 0.6921 - accuracy: 0.5229 - val_loss: 0.6926 - val_accuracy: 0.5178
    Epoch 14/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6920 - accuracy: 0.5244 - val_loss: 0.6922 - val_accuracy: 0.5220
    Epoch 15/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6921 - accuracy: 0.5233 - val_loss: 0.6923 - val_accuracy: 0.5202
    Epoch 16/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6920 - accuracy: 0.5244 - val_loss: 0.6935 - val_accuracy: 0.5089
    Epoch 17/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6922 - accuracy: 0.5218 - val_loss: 0.6925 - val_accuracy: 0.5181
    Epoch 18/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.6924 - accuracy: 0.5204 - val_loss: 0.6920 - val_accuracy: 0.5235
    Epoch 19/20
    180/180 [==============================] - 4s 21ms/step - loss: 0.6920 - accuracy: 0.5239 - val_loss: 0.6924 - val_accuracy: 0.5194
    Epoch 20/20
    180/180 [==============================] - 4s 22ms/step - loss: 0.6920 - accuracy: 0.5244 - val_loss: 0.6924 - val_accuracy: 0.5193



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])

#blue is training
#orange is validation
```




    [<matplotlib.lines.Line2D at 0x7fca601041d0>]




    
![output_36_1.png](/images/output_36_1.png)
    


### Model Evaluation


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"

test_data = pd.read_csv(test_url)
test_data

test = make_dataset(test_data)
```


```python
history = model2.fit(train, validation_data=test, epochs=20)
```

    Epoch 1/20
    180/180 [==============================] - 5s 28ms/step - loss: 0.1486 - accuracy: 0.9415 - val_loss: 0.1078 - val_accuracy: 0.9734
    Epoch 2/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1455 - accuracy: 0.9452 - val_loss: 0.1053 - val_accuracy: 0.9743
    Epoch 3/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1416 - accuracy: 0.9458 - val_loss: 0.1016 - val_accuracy: 0.9747
    Epoch 4/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1437 - accuracy: 0.9437 - val_loss: 0.0984 - val_accuracy: 0.9753
    Epoch 5/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1388 - accuracy: 0.9457 - val_loss: 0.0972 - val_accuracy: 0.9754
    Epoch 6/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1347 - accuracy: 0.9487 - val_loss: 0.0940 - val_accuracy: 0.9754
    Epoch 7/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1343 - accuracy: 0.9460 - val_loss: 0.0923 - val_accuracy: 0.9760
    Epoch 8/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1317 - accuracy: 0.9456 - val_loss: 0.0916 - val_accuracy: 0.9749
    Epoch 9/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1288 - accuracy: 0.9467 - val_loss: 0.0890 - val_accuracy: 0.9763
    Epoch 10/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1266 - accuracy: 0.9453 - val_loss: 0.0876 - val_accuracy: 0.9771
    Epoch 11/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1271 - accuracy: 0.9495 - val_loss: 0.0858 - val_accuracy: 0.9766
    Epoch 12/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1224 - accuracy: 0.9482 - val_loss: 0.0854 - val_accuracy: 0.9775
    Epoch 13/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1278 - accuracy: 0.9457 - val_loss: 0.0841 - val_accuracy: 0.9774
    Epoch 14/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1213 - accuracy: 0.9481 - val_loss: 0.0859 - val_accuracy: 0.9776
    Epoch 15/20
    180/180 [==============================] - 4s 23ms/step - loss: 0.1223 - accuracy: 0.9461 - val_loss: 0.0822 - val_accuracy: 0.9779
    Epoch 16/20
    180/180 [==============================] - 5s 28ms/step - loss: 0.1183 - accuracy: 0.9480 - val_loss: 0.0807 - val_accuracy: 0.9772
    Epoch 17/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1191 - accuracy: 0.9487 - val_loss: 0.0798 - val_accuracy: 0.9785
    Epoch 18/20
    180/180 [==============================] - 4s 25ms/step - loss: 0.1211 - accuracy: 0.9465 - val_loss: 0.0806 - val_accuracy: 0.9775
    Epoch 19/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1155 - accuracy: 0.9485 - val_loss: 0.0785 - val_accuracy: 0.9779
    Epoch 20/20
    180/180 [==============================] - 4s 24ms/step - loss: 0.1126 - accuracy: 0.9485 - val_loss: 0.0778 - val_accuracy: 0.9784



```python
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()

#blue is training
#orange is validation
```

    No handles with labels found to put in legend.





    <matplotlib.legend.Legend at 0x7fca61b62890>




    
![output_40_2.png](/images/output_40_2.png)
    


### Embedding Visualization


```python
weights = model2.get_layer('embedding2').get_weights()[0] # get the weights from the embedding layer
vocab = text_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = [2]*len(embedding_df),
                # size_max = 2,
                 hover_name = "word")

fig.show()
```

Five words that I found interpretable and interesting in this visualization are "apparently," "fox," "reportedly," "radical," and "21wire," all of which reside towards the left side of the visualization. "fox" and "21wire" most likely refers to Fox News and 21st Century Wire, which are both news sources that have been criticized for spreading propoganda and false or exhaggerated information. "apparently" and "reportedly" also make sense to me because these are words that are often used by writers when they can't be sure about the information; these words allow them to detach themselves from involvement. "radical" also makes sense as a fake news word because a lot of fake news articles tend to attack "radical leftists."

